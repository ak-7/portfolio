---
layout: page
permalink: /publications/
---

<div>
    <h2>Research & Services</h2>
    <h3><a href="#services">Services</a> | <a href="#book">Book</a> | <a href="#papers">Publications</a> | <a
            href="#datasets">Datasets</a></h3>
    <h3 id="services">Services</h3>
    <p>Served as Program Committee Member/Invited Reviewer at some of the leading conferences in Machine Learning:</p>
    <ul>
        <li> <a href="https://iclr.cc/">International Conference on Learning Representations (ICLR) 2024</a>
        <li> <a href="https://xai-sa-workshop.github.io/web/">ICASSP Explainable Machine Learning for Speech and Audio
                2024</a>
        <li> <a href="https://sites.google.com/view/aifin-aaai2024/home">AAAI 2024 workshop on AI in Finance for Social
                Impact</a>
        <li> <a
                href="https://www.credly.com/organizations/microsoft-machine-learning-ai-data-science-conference-mlads/badges">The
                Machine Learning, AI & Data Science Conference (MLADS) 2022, 2023, 2024</a>
    </ul>

    <h3 id="book">Book</h3>
    <p><a href="https://www.google.com/books/"><b>Generating Natural
                Questions from Images for Multimodal Assistants</b></a></p>

    <p> <b>Abstract</b>: Generating natural, diverse, and meaningful questions from images is an essential task for
        multimodal assistants as it
        confirms whether they have understood the object and scene in the images properly. The research in visual
        question
        answering (VQA) and visual question generation (VQG) is a great step. However, this research does not capture
        questions
        that a visually-abled person would ask multimodal assistants. Recently published datasets such as KB-VQA, FVQA,
        and
        OK-VQA try to collect questions that look for external knowledge which makes them appropriate for multimodal
        assistants.
        However, they still contain many obvious and common-sense questions that humans would not usually ask a digital
        assistant. In this paper, we provide a new benchmark dataset that contains questions generated by human
        annotators
        keeping in mind what they would ask multimodal digital assistants. Large scale annotations for thousands of
        images are
        expensive and time-consuming, so we also present an effective way of automatically generating questions from
        unseen
        images. In this paper, we present an approach for generating diverse and meaningful questions that consider
        image
        content and metadata of image (e.g., location, associated keyword). We evaluate our approach using standard
        evaluation
        metrics such as BLEU, METEOR, ROUGE, and CIDEr to show the relevance of generated questions with human-provided
        questions. We also measure the diversity of generated questions using generative strength and inventiveness
        metrics. We
        report new state-of-the-art results on the public and our datasets.</p>
    <!--     <center> <iframe type="text/html" width="336" height="550" frameborder="0" allowfullscreen style="max-width:100%" src="https://read.amazon.com/kp/card?asin=B08RN47C5T&preview=inline&linkCode=kpe&ref_=cm_sw_r_kb_dp_1mIeGbSZ013C3&tag=mobile0a1329f-20" ></iframe> </center> -->

    <h3 id="papers">Publications</h3>
    <ul>

        <li> <a href="https://ieeexplore.ieee.org/abstract/document/9413599"><b>ICASSP Conference</b></a>
            <ul>
                <li> <b>Generating Natural Questions from Images for Multimodal Assistants</b>
                <li> <a href="https://link.springer.com/chapter/10.1007/978-3-031-34006-2_2">Book Chapter</a>
                <li>
                    <details>
                        <summary>Citation Information</summary>
                        <br>
                        <b>Text format</b>: {% include publications title_search="Generating Natural Questions from
                        Images for Multimodal Assistants" style="mla" link=true %}
                        <br> <b>BibTex format</b>: <br>
                        <code>
                        @INPROCEEDINGS{9413599,
                        author={Patel, Alkesh and Bindal, Akanksha and Kotek, Hadas and Klein, Christopher and Williams, Jason},
                        booktitle={ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
                        title={Generating Natural Questions from Images for Multimodal Assistants},
                        year={2021},
                        volume={},
                        number={},
                        pages={2270-2274},
                        keywords={Visualization;Annotations;Conferences;Signal processing;Benchmark testing;Knowledge
                        discovery;Acoustics;Multimodal assistant;computer vision;visual question generation;long-short-term memory},
                        doi={10.1109/ICASSP39728.2021.9413599}}

                    </code>
                    </details>
            </ul>

    </ul>

    <h3 id="datasets">Datasets</h3>
    <ul>
        <li> <a href="https://github.com/apple/vqg-multimodal-assistant"><b>Generating Natural Questions from Images for
                    Multimodal Assistants</b></a> (<span style="color:red"><b>New</b></span>)
            <ul>
                <li> Generating natural, diverse, and meaningful questions from images is an essential task for
                    multimodal assistants as it
                    confirms whether they have understood the object and scene in the images properly. The research in
                    visual question
                    answering (VQA) and visual question generation (VQG) is a great step. However, this research does
                    not capture questions
                    that a visually-abled person would ask multimodal assistants. Recently published datasets such as
                    KB-VQA, FVQA, and
                    OK-VQA try to collect questions that look for external knowledge which makes them appropriate for
                    multimodal assistants.
                    However, they still contain many obvious and common-sense questions that humans would not usually
                    ask a digital
                    assistant. In this paper, we provide a new benchmark dataset that contains questions generated by
                    human annotators
                    keeping in mind what they would ask multimodal digital assistants. Large scale annotations for
                    thousands of images are
                    expensive and time-consuming, so we also present an effective way of automatically generating
                    questions from unseen
                    images. In this paper, we present an approach for generating diverse and meaningful questions that
                    consider image
                    content and metadata of image (e.g., location, associated keyword). We evaluate our approach using
                    standard evaluation
                    metrics such as BLEU, METEOR, ROUGE, and CIDEr to show the relevance of generated questions with
                    human-provided
                    questions. We also measure the diversity of generated questions using generative strength and
                    inventiveness metrics. We
                    report new state-of-the-art results on the public and our datasets.
                <li>
                <li>
                    <details>
                        <summary>Please cite these articles if you use the dataset</summary>
                        <br>
                        <b>Text format</b>: <br>
                        <ol>
                            <li> Generating Natural Questions from Images for Multimodal Assistants <i>ICASSP</i>
                                (2020).
                        </ol>
                        <br>
                        <b>BibTex format</b>: <br>
                        <code>
                        @INPROCEEDINGS{9413599,
                        author={Patel, Alkesh and Bindal, Akanksha and Kotek, Hadas and Klein, Christopher and Williams, Jason},
                        booktitle={ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
                        title={Generating Natural Questions from Images for Multimodal Assistants},
                        year={2021},
                        volume={},
                        number={},
                        pages={2270-2274},
                        keywords={Visualization;Annotations;Conferences;Signal processing;Benchmark testing;Knowledge
                        discovery;Acoustics;Multimodal assistant;computer vision;visual question generation;long-short-term memory},
                        doi={10.1109/ICASSP39728.2021.9413599}}
                            <br> <br>
                        </code>
                    </details>
            </ul>
    </ul>

    <!--      <h3 id="citations">Citations</h3>
     <ul>
         <li> <a href="https://scholar.google.com/citations?user={{ site.data.scholar.id }}"><b>Scholar Profile</b></a>
         <li> Citations: {{ site.data.scholar.citations }}
         <li> h-index: {{ site.data.scholar.h_index }}
         <li> i10-index: {{ site.data.scholar.i10_index }}
     </ul>  -->
</div>

<div>


</div>