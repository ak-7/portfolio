---
layout: page
permalink: /publications/
---

<div>
    <h2>Research</h2>
    <h3><a href="#services">Services</a> | <a href="#papers">Publications</a> | <a
            href="#datasets">Datasets</a></h3>
    <h3 id="services">Services</h3>
    <p>Served as Program Committee Member/Invited Reviewer at some of the leading conferences in Machine Learning:</p>
    <ul>
        <li> <a href="https://iclr.cc/">International Conference on Learning Representations (ICLR) 2024</a>
        <li> <a href="https://xai-sa-workshop.github.io/web/">ICASSP Explainable Machine Learning for Speech and Audio
                2024</a>
        <li> <a href="https://sites.google.com/view/aifin-aaai2024/home">AAAI 2024 workshop on AI in Finance for Social
                Impact</a>
        <li> <a
                href="https://www.credly.com/organizations/microsoft-machine-learning-ai-data-science-conference-mlads/badges">The
                Machine Learning, AI & Data Science Conference (MLADS) 2022, 2023, 2024</a>
    </ul>

    <h3 id="papers">Publications</h3>

    <ul>

        <li> <a href="https://arxiv.org/pdf/2405.11344"><b>Submitted to EMNLP Industry Track Conference</b></a>
            <ul>
                <li> <b>IMPROVED CONTENT UNDERSTANDING WITH EFFECTIVE USE OF MULTI-TASK
                        CONTRASTIVE LEARNING</b>
                <li>
                    <details>
                        <summary>Citation Information</summary>
                        <br>
                        <b>Text format</b>: {% include publications title_search="IMPROVED CONTENT UNDERSTANDING WITH
                        EFFECTIVE USE OF MULTI-TASK
                        CONTRASTIVE LEARNING" style="mla" link=true %}
                        <br> <b>BibTex format</b>: <br>
                        <code>
                            @misc{bindal2024improvedcontentunderstandingeffective,
                            title={Improved Content Understanding With Effective Use of Multi-task Contrastive Learning},
                            author={Akanksha Bindal and Sudarshan Ramanujam and Dave Golland and TJ Hazen and Tina Jiang and Fengyu Zhang and Peng
                            Yan},
                            year={2024},
                            eprint={2405.11344},
                            archivePrefix={arXiv},
                            primaryClass={cs.LG},
                            url={https://arxiv.org/abs/2405.11344},
                            }
                        </code>
                    </details>
            </ul>

    </ul>
    <ul>

        <li> <a href="https://ieeexplore.ieee.org/abstract/document/9413599"><b>ICASSP Conference</b></a>
            <ul>
                <li> <b>Streaming on-device detection of device directed speech from voice and touch-based invocation</b>
                <li>
                    <details>
                        <summary>Citation Information</summary>
                        <br>
                        <b>Text format</b>: {% include publications title_search="Streaming on-device detection of device directed speech from voice and touch-based invocation" style="mla" link=true %}
                        <br> <b>BibTex format</b>: <br>
                        <code>
                                @INPROCEEDINGS{9747107,
                                author={Rudovic, Ognjen Oggi and Bindal, Akanksha and Garg, Vineet and Simha, Pramod and Dighe, Pranay and Kajarekar,
                                Sachin},
                                booktitle={ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
                                title={Streaming on-Device Detection of Device Directed Speech from Voice and Touch-Based Invocation},
                                year={2022},
                                volume={},
                                number={},
                                pages={491-495},
                                keywords={Voice activity detection;Runtime;Computational modeling;Wearable computers;Virtual assistants;Signal
                                processing;Data models;smart assistant;false trigger mitigation;intent classification;streaming},
                                doi={10.1109/ICASSP43922.2022.9747107}}
                                </code>
                    </details>
            </ul>
            <ul>
                <li> <b>Generating Natural Questions from Images for Multimodal Assistants</b>
                <li>
                    <details>
                        <summary>Citation Information</summary>
                        <br>
                        <b>Text format</b>: {% include publications title_search="Generating Natural Questions from
                        Images for Multimodal Assistants" style="mla" link=true %}
                        <br> <b>BibTex format</b>: <br>
                        <code>
                        @INPROCEEDINGS{9413599,
                        author={Patel, Alkesh and Bindal, Akanksha and Kotek, Hadas and Klein, Christopher and Williams, Jason},
                        booktitle={ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
                        title={Generating Natural Questions from Images for Multimodal Assistants},
                        year={2021},
                        volume={},
                        number={},
                        pages={2270-2274},
                        keywords={Visualization;Annotations;Conferences;Signal processing;Benchmark testing;Knowledge
                        discovery;Acoustics;Multimodal assistant;computer vision;visual question generation;long-short-term memory},
                        doi={10.1109/ICASSP39728.2021.9413599}}

                    </code>
                    </details>
            </ul>

    </ul>

    <h3 id="datasets">Datasets</h3>
    <ul>
        <li> <a href="https://github.com/apple/vqg-multimodal-assistant"><b>Generating Natural Questions from Images for
                    Multimodal Assistants</b></a> (<span style="color:red"><b>New</b></span>)
            <ul>
                <li> Generating natural, diverse, and meaningful questions from images is an essential task for
                    multimodal assistants as it
                    confirms whether they have understood the object and scene in the images properly. The research in
                    visual question
                    answering (VQA) and visual question generation (VQG) is a great step. However, this research does
                    not capture questions
                    that a visually-abled person would ask multimodal assistants. Recently published datasets such as
                    KB-VQA, FVQA, and
                    OK-VQA try to collect questions that look for external knowledge which makes them appropriate for
                    multimodal assistants.
                    However, they still contain many obvious and common-sense questions that humans would not usually
                    ask a digital
                    assistant. In this paper, we provide a new benchmark dataset that contains questions generated by
                    human annotators
                    keeping in mind what they would ask multimodal digital assistants. Large scale annotations for
                    thousands of images are
                    expensive and time-consuming, so we also present an effective way of automatically generating
                    questions from unseen
                    images. In this paper, we present an approach for generating diverse and meaningful questions that
                    consider image
                    content and metadata of image (e.g., location, associated keyword). We evaluate our approach using
                    standard evaluation
                    metrics such as BLEU, METEOR, ROUGE, and CIDEr to show the relevance of generated questions with
                    human-provided
                    questions. We also measure the diversity of generated questions using generative strength and
                    inventiveness metrics. We
                    report new state-of-the-art results on the public and our datasets.
                <li>
                <li>
                    <details>
                        <summary>Please cite these articles if you use the dataset</summary>
                        <br>
                        <b>Text format</b>: <br>
                        <ol>
                            <li> Generating Natural Questions from Images for Multimodal Assistants <i>ICASSP</i>
                                (2020).
                        </ol>
                        <br>
                        <b>BibTex format</b>: <br>
                        <code>
                        @INPROCEEDINGS{9413599,
                        author={Patel, Alkesh and Bindal, Akanksha and Kotek, Hadas and Klein, Christopher and Williams, Jason},
                        booktitle={ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
                        title={Generating Natural Questions from Images for Multimodal Assistants},
                        year={2021},
                        volume={},
                        number={},
                        pages={2270-2274},
                        keywords={Visualization;Annotations;Conferences;Signal processing;Benchmark testing;Knowledge
                        discovery;Acoustics;Multimodal assistant;computer vision;visual question generation;long-short-term memory},
                        doi={10.1109/ICASSP39728.2021.9413599}}
                            <br> <br>
                        </code>
                    </details>
            </ul>
    </ul>

    <!--      <h3 id="citations">Citations</h3>
     <ul>
         <li> <a href="https://scholar.google.com/citations?user={{ site.data.scholar.id }}"><b>Scholar Profile</b></a>
         <li> Citations: {{ site.data.scholar.citations }}
         <li> h-index: {{ site.data.scholar.h_index }}
         <li> i10-index: {{ site.data.scholar.i10_index }}
     </ul>  -->
</div>

<div>


</div>