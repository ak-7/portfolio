---
layout: page
permalink: /projects/
---

<div>
    <div class="left">
        <!-- Generating Natural Questions from Images for Multimodal Assistants -->
        <div class="card">
            <a href="https://github.com/ak-7">
                <div class="card-image">
                    <img src="/images/projects/marketing_bias.png" alt="Orange" />
                </div>
            </a>
            <div class="card-body">
                <div class="card-title">
                    <h3>
                        Generating Natural Questions from Images for Multimodal Assistants
                    </h3>
                    <time>
                        May 2019 - December 2019
                    </time>
                </div>
                <div class="card-exceprt">
                    <p>
                        Generating natural, diverse, and meaningful questions from images is an essential task for
                        multimodal assistants as it
                        confirms whether they have understood the object and scene in the images properly. The research
                        in visual question
                        answering (VQA) and visual question generation (VQG) is a great step. However, this research
                        does not capture questions
                        that a visually-abled person would ask multimodal assistants. Recently published datasets such
                        as KB-VQA, FVQA, and
                        OK-VQA try to collect questions that look for external knowledge which makes them appropriate
                        for multimodal assistants.
                        However, they still contain many obvious and common-sense questions that humans would not
                        usually ask a digital
                        assistant. In this paper, we provide a new benchmark dataset that contains questions generated
                        by human annotators
                        keeping in mind what they would ask multimodal digital assistants. Large scale annotations for
                        thousands of images are
                        expensive and time-consuming, so we also present an effective way of automatically generating
                        questions from unseen
                        images. In this paper, we present an approach for generating diverse and meaningful questions
                        that consider image
                        content and metadata of image (e.g., location, associated keyword). We evaluate our approach
                        using standard evaluation
                        metrics such as BLEU, METEOR, ROUGE, and CIDEr to show the relevance of generated questions with
                        human-provided
                        questions. We also measure the diversity of generated questions using generative strength and
                        inventiveness metrics. We
                        report new state-of-the-art results on the public and our datasets.
                    </p>
                </div>
                <center>
                    <a class="btn" href="https://dl.acm.org/doi/pdf/10.1145/3336191.3371855">Paper</a>
                    <a class="btn" href="https://github.com/apple/vqg-multimodal-assistant">Datasets and
                        Implementation</a>
                </center>
            </div>
        </div>
    </div>

    <div class="right">

    </div>
</div>